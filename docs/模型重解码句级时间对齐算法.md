# 模型重解码句级时间对齐算法说明

本说明文档对应实现文件：`backend/tests/test_model_align_sentences.py`

该脚本通过“全局一次解码 + 分块重解码 + 文本拼接定位 + 比例映射修正”的流程，为中文音频生成句级的时间戳，并导出每句对应的音频切片。

---

## 目标

- 输入：一段中文音频（示例见 `AUDIO_PATH`），依赖离线 ASR（Paraformer+VAD+Punc）。
- 输出：
  - 句级时间索引 `backend/tests/results/model_align/sentence_times.json`
  - 前若干句切片 `backend/tests/results/model_align/clips/sent_XXX.m4a`

## 依赖

- 可执行依赖：`ffmpeg`（需在 PATH）
- Python 包：`torch`、`funasr`

---

## 核心思路

1. 全局解码得到：
   - `full_text`：全篇转写文本
   - `VAD` 段时间戳（毫秒级）
2. 将 VAD 段合并并切分为不超过 `MAX_CHUNK_S` 的块，设置相邻块重叠 `OVERLAP_S`。
3. 对每个块单独重解码得到 `chunk.text`，按启发式字符串重叠消除策略拼接为 `concat_text`，同时记录每个块在拼接文本中的区间 `concat_ranges`。
4. 将 `full_text` 进行中文分句（按 `。！？；`），逐句去 `concat_text` 中查找其位置（失败时用前若干字锚点或比例近似）。
5. 使用“文本位置在线性插值为时间”的方法，将句子在 `concat_text` 的起止索引映射为音频时间。
6. 做边界与单调性修正、padding，输出稳定的句级时间段并切片。

---

## 处理流程（与代码主流程一一对应）

- 0) 加载模型（与现有配置保持一致）
  - `AutoModel`：Paraformer 大模型 + `fsmn-vad` + `ct-punc-c`
- 1) 全局一次解码
  - `model.generate(input=AUDIO_PATH, ...)` → 取 `text` 与 `timestamp`（VAD 段）
- 2) VAD → chunk
  - 合并近邻（间隔 ≤ `MERGE_GAP_S`）
  - 限制单块最长 `MAX_CHUNK_S`，相邻块加入 `OVERLAP_S` 重叠
- 3) 逐块重解码
  - 通过 `ffmpeg` 裁切音频块，分别调用 `model.generate` 获取 `text`
- 4) 文本拼接
  - 启发式重叠对齐：比较 `concat_text` 尾部与新块文本头部的最长“后缀-前缀”重叠（`longest_common_suffix_prefix`）
  - 记录每个块在拼接文本中的 `[c_start, c_end)`
- 5) 句子定位
  - `split_cn_sentences(full_text)` → 句子列表
  - 在 `concat_text` 中查找句子：优先 `substring`，失败退化到“句首锚点”或“比例近似”
- 6) 文本位置 → 时间
  - 对任意文本位置 `p`，找到其落入的块区间 `(c_start, c_end)`，对块的时间 `[start_s, end_s]` 做线性插值：
    - `t = start_s + (p - c_start) / (c_end - c_start) * (end_s - start_s)`
  - 边界情况（落在任一块外）做夹逼或近邻选取
- 7) 边界与单调修正
  - `PAD_START_S` / `PAD_END_S`：在句首/句尾加微小 padding
  - 保证非降时间、最小时长（如不足则拉长到 0.12s）
- 8) 导出
  - 写 `sentence_times.json`
  - 切出前 `EXPORT_TOP_N` 句音频片段

---

## 关键函数

- `split_cn_sentences(text)`
  - 通过正则 `[。！？；]` 分句，返回 `(start, end, sent)` 列表（基于 `text` 的字符索引）
- `ms_segments_from_item(item)`
  - 从全局解码结果中提取毫秒级 VAD 段数组
- `merge_ms_to_chunks(ms_segs)`
  - 合并近邻段，限制最大时长，注入重叠，返回秒级 `(start_s, end_s)` 块列表
- `ffmpeg_cut(in_path, start_s, end_s, out_path)`
  - 用 ffmpeg 裁切音频块
- `longest_common_suffix_prefix(a, b, max_k)`
  - 计算字符串 `a` 的后缀与 `b` 的前缀的最长重叠长度（上限 `max_k`）
- `pos_time(p)`（内嵌于主流程第 6 步）
  - 将拼接文本位置 `p` 映射为时间，落入块内采用线性插值；块外采用夹逼/近邻中心

---

## 重要参数（位于脚本顶部）

- 路径
  - `AUDIO_PATH`：输入音频
  - `RESULT_DIR` / `CLIPS_DIR` / `TMP_DIR`：输出与中间目录
- chunk 控制
  - `MAX_CHUNK_S = 30.0`：单块最大秒数
  - `MERGE_GAP_S = 0.6`：VAD 段合并阈值（秒）
  - `OVERLAP_S = 0.8`：相邻块重叠时长（秒）
- 导出/边界
  - `EXPORT_TOP_N = 10`：导出前 N 句音频切片
  - `PAD_START_S = 0.10` / `PAD_END_S = 0.20`：句首/句尾 padding

---

## 运行方式

- 直接运行测试脚本（无需 argparse）：

```bash
python backend/tests/test_model_align_sentences.py
```

- 运行前确认：
  - `AUDIO_PATH` 指向存在的音频文件
  - 已安装 `ffmpeg` 并可在命令行访问
  - 已安装 `torch`、`funasr` 且能加载所需模型（默认会自动下载）

---

## 复杂度与性能

- 解码复杂度主要由 ASR 推理决定。
- 分块重解码相对全局一次解码增加了推理次数，但块更短，延迟更可控；并能提升句级定位的鲁棒性。

---

## 误差来源与可能改进

- 文本不完全一致：全局解码与分块解码可能因上下文不同出现轻微差异 → 通过“后缀-前缀重叠”与锚点回退缓解。
- 字符查找失败：极端情况下使用比例近似，可能带来小幅定位偏差。
- 线性插值假设：假设文本在块内按字符均匀分布到时间，快速有效但非严格精确。

可选增强：

- 采用更强的文本对齐（如 CTC 对齐或动态规划基于编辑距离的细粒度对齐）。
- 在块内利用词级时间戳（若模型可提供）替代线性插值。
- 自适应调节 `OVERLAP_S` 与拼接重叠窗口大小。

---

## 输出示例

- 终端：
  - `ok: .../clips/sent_001.m4a [12.340s ~ 15.620s]`
  - `索引: .../sentence_times.json`
- JSON：数组，每个元素包含：
  - `sentence`，`start_s`，`end_s`，`via="model_align"`

---

## 目录与文件

- 实现脚本：`backend/tests/test_model_align_sentences.py`
- 结果目录：`backend/tests/results/model_align/`
  - `sentence_times.json`
  - `clips/` 中的 `sent_*.m4a`
  - `tmp/` 中间块音频
